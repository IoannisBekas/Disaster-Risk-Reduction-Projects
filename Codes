###############################################################################################################

                                                  PROJECT 1
                                           Tropical Storm Forming

                 This code predicts tropical storm formation within the next 7 days using a 
               logistic regression model and assigns risk alerts based on probability thresholds.

###############################################################################################################

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. Create the DataFrame directly
data = {
    "week_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
    "mean_sea_level_pressure": [1013, 1010, 1012, 1008, 1014, 1005, 1011, 1007, 1013, 1006, 1012, 1009],
    "sea_surface_temp": [28.5, 29.0, 27.5, 29.2, 27.0, 29.8, 28.2, 29.5, 28.0, 29.6, 28.3, 29.3],
    "wind_speed": [15, 18, 12, 20, 10, 22, 14, 19, 15, 21, 13, 17],
    "humidity": [75, 80, 70, 85, 65, 88, 73, 82, 75, 86, 72, 80],
    "region": ["Coastal_North"] * 12,
    "storm_formed": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
}

df = pd.DataFrame(data)

# Feature Engineering
df['sst_lag_1'] = df['sea_surface_temp'].shift(1)
df['pressure_lag_1'] = df['mean_sea_level_pressure'].shift(1)
df.dropna(inplace=True)

# Define Features and Target
feature_cols = ['mean_sea_level_pressure', 'sea_surface_temp', 'wind_speed', 'humidity', 'sst_lag_1', 'pressure_lag_1']
X = df[feature_cols]
y = df['storm_formed']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression Model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predictions
y_pred = logreg.predict(X_test)

# Evaluation
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Predict Probabilities
y_prob = logreg.predict_proba(X_test)[:, 1]

def assign_alert_level(prob):
    if prob < 0.2:
        return "Green"
    elif prob < 0.4:
        return "Yellow"
    elif prob < 0.6:
        return "Orange"
    else:
        return "Red"

alerts = [assign_alert_level(p) for p in y_prob]
for i, alert in enumerate(alerts):
    print(f"Test sample {i} - Probability: {y_prob[i]:.2f} - Alert Level: {alert}")

# Visualization
plt.scatter(range(len(y_test)), y_prob, c=y_test, cmap='bwr', alpha=0.7)
plt.axhline(0.6, color='red', linestyle='--', label='Red alert threshold (0.6)')
plt.axhline(0.4, color='orange', linestyle='--', label='Orange alert threshold (0.4)')
plt.axhline(0.2, color='yellow', linestyle='--', label='Yellow alert threshold (0.2)')
plt.title('Predicted Probability of Storm Formation')
plt.xlabel('Test Sample Index')
plt.ylabel('Probability')
plt.legend()
plt.show()




###############################################################################################################

                                                  PROJECT 2
                                           Drought Risk Forecasting
This code predicts drought risk by analyzing time-series weather data using an LSTM deep learning model. 
It processes historical temperature, rainfall, and soil moisture levels to generate drought index forecasts, 
                            enabling early warning and disaster preparedness.

###############################################################################################################


"""
advanced_drr_time_series.py

Demonstration of an advanced, end-to-end Python AI project for DRR:

1) Synthetic Data Creation (time-series weather + drought index).
2) Data Cleaning + Feature Engineering.
3) LSTM Model Training (Keras).
4) Model Evaluation (metrics + visualization).
5) Model Saving (both .keras format and TF SavedModel) + Simple Flask Deployment Example.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Flask for deployment
from flask import Flask, request, jsonify

# ------------------------------------------
# 1. Create Synthetic Time-Series Data
# ------------------------------------------

def create_synthetic_drought_data(num_weeks=200):
    """
    Generate synthetic weekly data with columns:
     - date (Weekly intervals)
     - rainfall (mm)
     - temperature (°C)
     - soil_moisture (%)
     - drought_index (0-5 scale)
    to simulate cyclical weather patterns for DRR forecasting.
    """
    start_date = datetime.date(2022, 1, 1)
    dates = [start_date + datetime.timedelta(weeks=i) for i in range(num_weeks)]

    rng = np.random.default_rng(seed=42)
    rainfall = 50 + 30 * np.sin(np.linspace(0, 3 * np.pi, num_weeks)) + rng.normal(0, 5, size=num_weeks)
    temperature = 25 + 5 * np.sin(np.linspace(0, 2 * np.pi, num_weeks) + np.pi/3) + rng.normal(0, 1, size=num_weeks)
    soil_moisture = 40 + 10 * np.cos(np.linspace(0, 2 * np.pi, num_weeks) + np.pi/6) + rng.normal(0, 3, size=num_weeks)

    # A rough formula for "drought_index"
    base_drought = 2.5 - 0.02*rainfall - 0.03*soil_moisture + 0.01*temperature
    drought_index = base_drought + rng.normal(0, 0.5, size=num_weeks)
    drought_index = np.clip(drought_index, 0, 5)

    df = pd.DataFrame({
        "date": dates,
        "rainfall": rainfall,
        "temperature": temperature,
        "soil_moisture": soil_moisture,
        "drought_index": drought_index
    })
    return df

# ------------------------------------------
# 2. Data Cleaning & Feature Engineering
# ------------------------------------------

def clean_and_prepare_data(df):
    """
    1. Clip negative rainfall to zero.
    2. Fill missing columns with mean if found.
    3. Sort by date.
    4. Return cleaned DataFrame.
    """
    df["rainfall"] = df["rainfall"].clip(lower=0)
    for col in ["rainfall", "temperature", "soil_moisture", "drought_index"]:
        if df[col].isna().sum() > 0:
            df[col].fillna(df[col].mean(), inplace=True)
    df.sort_values("date", inplace=True)
    df.reset_index(drop=True, inplace=True)
    return df

def create_time_series_windows(df, input_cols, target_col, window_size=4, forecast_horizon=4):
    """
    Convert the DataFrame into a supervised learning format for LSTM:
      - window_size of past steps
      - forecast_horizon steps ahead for the target
    Returns:
      X (num_samples, window_size, num_features)
      y (num_samples,)
    """
    data = df[input_cols + [target_col]].values
    X_list, y_list = [], []
    num_points = len(data)

    for i in range(num_points - window_size - forecast_horizon):
        X_window = data[i : i + window_size, :-1]  # everything except last col => input features
        y_target = data[i + window_size + forecast_horizon - 1, -1]  # last col => target
        X_list.append(X_window)
        y_list.append(y_target)

    return np.array(X_list), np.array(y_list)

# ------------------------------------------
# 3. Build & Train LSTM Model
# ------------------------------------------

def build_lstm_model(input_shape):
    """
    Construct a simple Keras LSTM architecture:
     - LSTM(64) with dropout
     - Dense(32) + ReLU
     - Dense(1) linear for a single regression output
    """
    model = Sequential()
    model.add(LSTM(64, input_shape=input_shape, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model


###############################################################################################################

                                                  PROJECT 3
                                          Flood Risk Prediction System
Predicting flood risk based on environmental factors such as rainfall, soil moisture, river levels, temperature, 
and region elevation. Using a custom neural network (MLP) built with TensorFlow, it assesses the probability 
             of flood occurrence and enables early warning for disaster risk reduction.
###############################################################################################################

"""
flood_risk_tensorflow.py

End-to-end TensorFlow script (no Keras high-level API), specifically for DRR (Flood Risk):
1) Synthetic Data Creation (flood risk classification).
2) tf.data Pipeline.
3) Custom Neural Network with tf.Module & Variables.
4) Manual Training Loop with GradientTape.
5) Model Saving & Loading (tf.saved_model) in a fully trackable manner.
"""

import numpy as np
import tensorflow as tf

# ------------------------------------------
# 1. Generate Synthetic Data
# ------------------------------------------

def generate_flood_data(num_samples=2000):
    """
    Create synthetic features that might affect flood risk:
      - rainfall (mm)
      - soil_moisture (%)
      - river_level (m)
      - temperature (°C)
      - region_elevation (m)
    We'll produce a binary label: flood_occurred (0 or 1).
    """

    np.random.seed(42)
    # 5 feature columns
    rainfall = np.random.uniform(0, 300, size=num_samples)
    soil_moisture = np.random.uniform(0, 100, size=num_samples)
    river_level = np.random.uniform(0, 10, size=num_samples)
    temperature = np.random.uniform(15, 35, size=num_samples)
    region_elevation = np.random.uniform(0, 500, size=num_samples)

    features = np.stack(
        [rainfall, soil_moisture, river_level, temperature, region_elevation],
        axis=1
    )

    # Synthetic flood_score
    flood_score = (
        0.03 * rainfall
        + 0.02 * soil_moisture
        + 0.5 * river_level
        - 0.005 * region_elevation
    )
    noise = np.random.normal(0, 1, num_samples)
    combined = flood_score + noise

    threshold = 6.0
    labels = (combined > threshold).astype(np.int32)

    return features, labels

# ------------------------------------------
# 2. Create tf.data Pipeline
# ------------------------------------------

def make_dataset(features, labels, batch_size=32, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices((features, labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(features))
    ds = ds.batch(batch_size)
    return ds

# ------------------------------------------
# 3. Build a TF Module (Raw NN)
# ------------------------------------------

class FloodRiskModel(tf.Module):
    """
    A raw multi-layer perceptron for binary classification,
    trackable by TensorFlow's SavedModel because we inherit from tf.Module
    and store variables as attributes.
    """

    def __init__(self, input_dim=5, hidden_dim1=16, hidden_dim2=8, name=None):
        super().__init__(name=name)
        
        # Use tf.random.normal, ensure dtype float32
        w_init1 = tf.random.normal([input_dim, hidden_dim1], stddev=0.1, dtype=tf.float32)
        b_init1 = tf.zeros([hidden_dim1], dtype=tf.float32)

        w_init2 = tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1, dtype=tf.float32)
        b_init2 = tf.zeros([hidden_dim2], dtype=tf.float32)

        w_out = tf.random.normal([hidden_dim2, 1], stddev=0.1, dtype=tf.float32)
        b_out = tf.zeros([1], dtype=tf.float32)

        # Declare them as tracked tf.Variables
        self.W1 = tf.Variable(w_init1, name="W1")
        self.b1 = tf.Variable(b_init1, name="b1")

        self.W2 = tf.Variable(w_init2, name="W2")
        self.b2 = tf.Variable(b_init2, name="b2")

        self.Wout = tf.Variable(w_out, name="Wout")
        self.bout = tf.Variable(b_out, name="bout")

    def forward(self, x):
        """
        Forward pass for raw NN: x shape (batch_size, input_dim)
        Returns shape (batch_size, 1) (the logits).
        """
        x = tf.cast(x, tf.float32)  # ensure float32
        h1 = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)
        h2 = tf.nn.relu(tf.matmul(h1, self.W2) + self.b2)
        logits = tf.matmul(h2, self.Wout) + self.bout
        return logits

    @tf.function(input_signature=[tf.TensorSpec([None, None], dtype=tf.float32)])
    def predict_logits(self, x):
        """
        SavedModel signature for returning raw logits.
        """
        return self.forward(x)

    @tf.function(input_signature=[tf.TensorSpec([None, None], dtype=tf.float32)])
    def predict_prob(self, x):
        """
        SavedModel signature for returning probability (sigmoid).
        """
        logits = self.forward(x)
        return tf.sigmoid(logits)

    @property
    def trainable_variables(self):
        """
        Return list of trainable variables for optimizer.
        (Alternatively, we could rely on self.variables, 
         but let's be explicit.)
        """
        return [self.W1, self.b1, self.W2, self.b2, self.Wout, self.bout]

# ------------------------------------------
# 4. Training & Evaluation Loop
# ------------------------------------------

def train_step(model, x_batch, y_batch, optimizer):
    x_batch = tf.cast(x_batch, tf.float32)
    y_batch = tf.cast(y_batch, tf.float32)

    with tf.GradientTape() as tape:
        logits = model.forward(x_batch)
        loss = tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.squeeze(y_batch), logits=tf.squeeze(logits)
        )
        loss = tf.reduce_mean(loss)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss

def compute_accuracy(model, dataset):
    correct = 0
    total = 0
    for x_batch, y_batch in dataset:
        logits = model.forward(x_batch)
        probs = tf.sigmoid(logits)
        preds = tf.cast(probs > 0.5, tf.int32)
        
        # y_batch shape might be (batch_size,) => reshape if needed
        y_batch = tf.cast(tf.reshape(y_batch, [-1, 1]), tf.int32)
        eq = tf.cast(preds == y_batch, tf.int32)
        correct += int(tf.reduce_sum(eq))
        total += x_batch.shape[0]
    return correct / total

# ------------------------------------------
# Main Script
# ------------------------------------------

if __name__ == "__main__":
    # Hyperparams
    BATCH_SIZE = 32
    EPOCHS = 5
    LR = 1e-3

    # 1) Generate Synthetic Data
    features, labels = generate_flood_data(num_samples=2000)
    # Convert to float32
    features = features.astype(np.float32)

    # 2) Train-Test Split
    split_idx = int(0.8 * len(features))
    train_features, test_features = features[:split_idx], features[split_idx:]
    train_labels, test_labels = labels[:split_idx], labels[split_idx:]

    # 3) Create tf.data Datasets
    train_ds = make_dataset(train_features, train_labels, BATCH_SIZE, shuffle=True)
    test_ds = make_dataset(test_features, test_labels, BATCH_SIZE, shuffle=False)

    # 4) Build Model
    input_dim = train_features.shape[1]  # 5
    model = FloodRiskModel(input_dim=input_dim, hidden_dim1=32, hidden_dim2=16)

    # 5) Training Loop
    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)

    for epoch in range(EPOCHS):
        epoch_loss = 0.0
        steps = 0
        for x_batch, y_batch in train_ds:
            loss_val = train_step(model, x_batch, y_batch, optimizer)
            epoch_loss += float(loss_val)
            steps += 1

        # Evaluate
        train_acc = compute_accuracy(model, make_dataset(train_features, train_labels, BATCH_SIZE, shuffle=False))
        test_acc = compute_accuracy(model, test_ds)

        print(f"Epoch {epoch+1}/{EPOCHS}, Loss={epoch_loss/steps:.4f}, "
              f"Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}")

    # Final Test Accuracy
    final_acc = compute_accuracy(model, test_ds)
    print(f"\nFinal Test Accuracy = {final_acc:.4f}\n")

    # 6) Save as TF SavedModel
    save_dir = "flood_risk_savedmodel"
    tf.saved_model.save(model, save_dir)
    print(f"Saved model to: {save_dir}")

    # 7) Demonstrate reloading & inference
    print("\nReloading model for test inference:")
    loaded_model = tf.saved_model.load(save_dir)

    # The loaded model has 'predict_prob' and 'predict_logits'
    # defined by the @tf.function decorators with signatures
    sample_input = test_features[:5]  # shape (5, 5)
    sample_probs = loaded_model.predict_prob(sample_input)
    preds = tf.cast(sample_probs > 0.5, tf.int32).numpy().flatten()

    print("Sample input:\n", sample_input)
    print("Probabilities:\n", sample_probs.numpy().flatten())
    print("Predicted classes:\n", preds)
    print("Ground truth:\n", test_labels[:5])









